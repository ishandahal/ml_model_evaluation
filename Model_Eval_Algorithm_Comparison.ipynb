{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Eval_Algorithm_Comparison.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMYIHQY0C3soG5tQYi/I7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/ml_model_evaluation/blob/main/Model_Eval_Algorithm_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOyYGGcDDJ8K"
      },
      "source": [
        "- Implementing nested cross-validation in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8uJPwp2DcF3"
      },
      "source": [
        "pip install watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njYUEjgZDfnn",
        "outputId": "5bca0044-21f8-46bc-c092-7da11e8e99b8"
      },
      "source": [
        "%load_ext watermark\n",
        "%watermark -a 'ishan dahal' -d -p sklearn,mlxtend -v"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ishan dahal 2020-11-26 \n",
            "\n",
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "sklearn 0.0\n",
            "mlxtend 0.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThvN4sXBMuIc"
      },
      "source": [
        "- Setting up classifiers and the parameters for model tuning\n",
        "- Hyperparameter tuning takes place in the inner loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAgs_ZnDrcJ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from mlxtend.data import mnist_data\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X, y = mnist_data()\n",
        "X = X.astype(np.float32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=y)\n",
        "# Initalizing Classifiers\n",
        "clf1 = LogisticRegression(multi_class='multinomial',\n",
        "                          solver='newton-cg',\n",
        "                          random_state=1)\n",
        "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
        "                            leaf_size=50)\n",
        "clf3 = DecisionTreeClassifier(random_state=1)\n",
        "clf4 = SVC(random_state=1)\n",
        "clf5 = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Building the pipelines\n",
        "pipe1 = Pipeline([('std', StandardScaler()),\n",
        "                  ('clf1', clf1)])\n",
        "\n",
        "pipe2 = Pipeline([('std', StandardScaler()),\n",
        "                  ('clf2', clf2)])\n",
        "\n",
        "pipe4 = Pipeline([('std', StandardScaler()),\n",
        "                  ('clf4', clf4)])\n",
        "\n",
        "# Setting up the parameter grids\n",
        "param_grid1 = [{'clf1__penalty': ['l2'],\n",
        "                'clf1__C': np.power(10., np.arange(-4, 4))}]\n",
        "\n",
        "param_grid2 = [{'clf2__n_neighbors': list(range(1, 10)),\n",
        "                'clf2__p': [1, 2]}]\n",
        "\n",
        "param_grid3 = [{'max_depth': list(range(1, 10)) + [None],\n",
        "                'criterion': ['gini', 'entropy']}]\n",
        "\n",
        "param_grid4 = [{'clf4__kernel': ['rbf'],\n",
        "                'clf4__C': np.power(10., np.arange(-4, 4)),\n",
        "                'clf4__gamma': np.power(10., np.arange(-5, 0))},\n",
        "               {'clf4__kernel': ['linear'],\n",
        "                'clf4__C': np.power(10., np.arange(-4, 4))}]\n",
        "\n",
        "param_grid5 = [{'n_estimators': [10, 100, 500, 1000, 10000]}]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbgBfqI3D35a"
      },
      "source": [
        "## Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
        "gridcvs = {}\n",
        "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
        "\n",
        "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4,\n",
        "                             param_grid5), (pipe1, pipe2, clf3, pipe4, clf5),\n",
        "                            ('Softmax', 'KNN', 'DTree', 'SVM', 'RForest')):\n",
        "    gcv = GridSearchCV(estimator=est,\n",
        "                       param_grid=pgrid,\n",
        "                       scoring='accuracy',\n",
        "                       n_jobs=-1,\n",
        "                       cv=inner_cv,\n",
        "                       verbose=0,\n",
        "                       refit=True)\n",
        "    gridcvs[name] = gcv"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ3rVgXCD59x"
      },
      "source": [
        "- Next, we define the outer loop\n",
        "- The training folds from the outer loop will be used in the inner loop for model tuning\n",
        "- The inner loop selects the best hyperparameter setting\n",
        "- This best hyperparameter setting can be evaluated on both the avg. over the inner test folds and the 1 corresponding test fold of the outer loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqoZ4cIrTaM0",
        "outputId": "d885ec62-9c9a-4af0-d666-d2c1aa1f472f"
      },
      "source": [
        "for name, gs_est in sorted(gridcvs.items()):\n",
        "\n",
        "    print(50 * '-', '\\n')\n",
        "    print('Algorithm: ', name)\n",
        "    print('    Inner loop:')\n",
        "\n",
        "    outer_scores = []\n",
        "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "    for train_idx, valid_idx in outer_cv.split(X_train, y_train):\n",
        "\n",
        "        gridcvs[name].fit(X_train[train_idx], y_train[train_idx]) # inner loop for hyperparameter tuning\n",
        "        print(f'\\n       Best ACC (avg. of inner test folds) {gridcvs[name].best_score_ * 100:.2f}%')\n",
        "        print('         Best parameters: ', gridcvs[name].best_params_)\n",
        "        ## perform on test fold (valid_idx)\n",
        "        outer_scores.append(gridcvs[name].best_estimator_.score(X_train[valid_idx], y_train[valid_idx]))\n",
        "        print(f'         ACC (on outer test fold) {outer_scores[-1] * 100}')\n",
        "\n",
        "        print('\\n       Outer Loop:')\n",
        "        print(f'            ACC {np.mean(outer_scores) * 100:.2f}% +/- {np.std(outer_scores) * 100:.2f}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------- \n",
            "\n",
            "Algorithm:  DTree\n",
            "    Inner loop:\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 72.59%\n",
            "         Best parameters:  {'criterion': 'gini', 'max_depth': None}\n",
            "         ACC (on outer test fold) 75.5\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 75.50% +/- 0.00\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 74.03%\n",
            "         Best parameters:  {'criterion': 'entropy', 'max_depth': 7}\n",
            "         ACC (on outer test fold) 78.25\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 76.88% +/- 1.37\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 73.88%\n",
            "         Best parameters:  {'criterion': 'entropy', 'max_depth': 9}\n",
            "         ACC (on outer test fold) 77.375\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 77.04% +/- 1.15\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 73.38%\n",
            "         Best parameters:  {'criterion': 'entropy', 'max_depth': 8}\n",
            "         ACC (on outer test fold) 74.875\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 76.50% +/- 1.37\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 73.91%\n",
            "         Best parameters:  {'criterion': 'entropy', 'max_depth': 8}\n",
            "         ACC (on outer test fold) 77.75\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 76.75% +/- 1.32\n",
            "-------------------------------------------------- \n",
            "\n",
            "Algorithm:  KNN\n",
            "    Inner loop:\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 88.38%\n",
            "         Best parameters:  {'clf2__n_neighbors': 1, 'clf2__p': 1}\n",
            "         ACC (on outer test fold) 91.625\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.62% +/- 0.00\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 88.75%\n",
            "         Best parameters:  {'clf2__n_neighbors': 1, 'clf2__p': 1}\n",
            "         ACC (on outer test fold) 91.875\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.75% +/- 0.12\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 89.84%\n",
            "         Best parameters:  {'clf2__n_neighbors': 1, 'clf2__p': 1}\n",
            "         ACC (on outer test fold) 90.875\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.46% +/- 0.42\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 89.50%\n",
            "         Best parameters:  {'clf2__n_neighbors': 1, 'clf2__p': 1}\n",
            "         ACC (on outer test fold) 90.875\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.31% +/- 0.45\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 89.06%\n",
            "         Best parameters:  {'clf2__n_neighbors': 1, 'clf2__p': 1}\n",
            "         ACC (on outer test fold) 90.25\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.10% +/- 0.58\n",
            "-------------------------------------------------- \n",
            "\n",
            "Algorithm:  RForest\n",
            "    Inner loop:\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 92.59%\n",
            "         Best parameters:  {'n_estimators': 1000}\n",
            "         ACC (on outer test fold) 95.0\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 95.00% +/- 0.00\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 92.59%\n",
            "         Best parameters:  {'n_estimators': 10000}\n",
            "         ACC (on outer test fold) 94.75\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 94.88% +/- 0.12\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 92.94%\n",
            "         Best parameters:  {'n_estimators': 10000}\n",
            "         ACC (on outer test fold) 94.5\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 94.75% +/- 0.20\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 93.00%\n",
            "         Best parameters:  {'n_estimators': 10000}\n",
            "         ACC (on outer test fold) 92.5\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 94.19% +/- 0.99\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 92.75%\n",
            "         Best parameters:  {'n_estimators': 500}\n",
            "         ACC (on outer test fold) 93.125\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 93.98% +/- 0.98\n",
            "-------------------------------------------------- \n",
            "\n",
            "Algorithm:  SVM\n",
            "    Inner loop:\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 90.75%\n",
            "         Best parameters:  {'clf4__C': 10.0, 'clf4__gamma': 0.001, 'clf4__kernel': 'rbf'}\n",
            "         ACC (on outer test fold) 92.125\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 92.12% +/- 0.00\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 90.22%\n",
            "         Best parameters:  {'clf4__C': 0.01, 'clf4__kernel': 'linear'}\n",
            "         ACC (on outer test fold) 92.875\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 92.50% +/- 0.37\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 90.91%\n",
            "         Best parameters:  {'clf4__C': 0.01, 'clf4__kernel': 'linear'}\n",
            "         ACC (on outer test fold) 90.5\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.83% +/- 0.99\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 90.53%\n",
            "         Best parameters:  {'clf4__C': 10.0, 'clf4__gamma': 0.001, 'clf4__kernel': 'rbf'}\n",
            "         ACC (on outer test fold) 92.75\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 92.06% +/- 0.95\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 90.12%\n",
            "         Best parameters:  {'clf4__C': 0.001, 'clf4__kernel': 'linear'}\n",
            "         ACC (on outer test fold) 90.75\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 91.80% +/- 1.00\n",
            "-------------------------------------------------- \n",
            "\n",
            "Algorithm:  Softmax\n",
            "    Inner loop:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       Best ACC (avg. of inner test folds) 88.91%\n",
            "         Best parameters:  {'clf1__C': 0.01, 'clf1__penalty': 'l2'}\n",
            "         ACC (on outer test fold) 90.0\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 90.00% +/- 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       Best ACC (avg. of inner test folds) 88.75%\n",
            "         Best parameters:  {'clf1__C': 0.01, 'clf1__penalty': 'l2'}\n",
            "         ACC (on outer test fold) 91.0\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 90.50% +/- 0.50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       Best ACC (avg. of inner test folds) 89.31%\n",
            "         Best parameters:  {'clf1__C': 0.01, 'clf1__penalty': 'l2'}\n",
            "         ACC (on outer test fold) 90.0\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 90.33% +/- 0.47\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 88.59%\n",
            "         Best parameters:  {'clf1__C': 0.1, 'clf1__penalty': 'l2'}\n",
            "         ACC (on outer test fold) 89.375\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 90.09% +/- 0.58\n",
            "\n",
            "       Best ACC (avg. of inner test folds) 88.66%\n",
            "         Best parameters:  {'clf1__C': 0.01, 'clf1__penalty': 'l2'}\n",
            "         ACC (on outer test fold) 89.5\n",
            "\n",
            "       Outer Loop:\n",
            "            ACC 89.97% +/- 0.57\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XR6bH6VWjXC"
      },
      "source": [
        "- Determine the best algorithm from the experiment above; e.g, we find the Random Forest is performing the best\n",
        "- Now select hyperparameters for the model based on regular k-fold on the whole training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orw3lLammsJG",
        "outputId": "e4e6ad39-79b9-4721-809b-531125d0fcbc"
      },
      "source": [
        "gcv_model_select = GridSearchCV(estimator=clf5,\n",
        "                                param_grid=param_grid5,\n",
        "                                scoring='accuracy',\n",
        "                                n_jobs=-1,\n",
        "                                cv=inner_cv,\n",
        "                                verbose=1,\n",
        "                                refit=True)\n",
        "\n",
        "gcv_model_select.fit(X_train, y_train)\n",
        "print(f'Best CV accuracy: {gcv_model_select.best_score_ * 100:.2f}%')\n",
        "print('Best parameters:', gcv_model_select.best_params_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best CV accuracy: 93.30%\n",
            "Best parameters: {'n_estimators': 10000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQOIBeinTI-"
      },
      "source": [
        "- Using these settings, we can now train the best model to the whole training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iuMWJxfngfd",
        "outputId": "d3a8f6a4-b108-4b7e-9d75-207b41247f8c"
      },
      "source": [
        "# we can skip refitting the model because we set refit=True\n",
        "# so sklearn has already fit the mdoel to the entire dataset\n",
        "\n",
        "train_acc = accuracy_score(y_true=y_train, y_pred=gcv_model_select.predict(X_train))\n",
        "test_acc = accuracy_score(y_true=y_test, y_pred=gcv_model_select.predict(X_test))\n",
        "\n",
        "print(f\"Training Accuracy: {100 * train_acc:.2f}%\")\n",
        "print(f\"Test Accuracy: {100 * test_acc:.2f}%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 100.00%\n",
            "Test Accuracy: 94.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-hLzZCosS3"
      },
      "source": [
        "The outer fold accuracy previously was ACC: 93.98% +/- 0.98, which is pretty close to the what we have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exFJ0eIspsRm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}